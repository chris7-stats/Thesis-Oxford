Code taken from :https://medium.com/correll-lab/building-clip-from-scratch-68f6e42d35f4 but changes have been made

from typing import Dict, Tuple
from tqdm import tqdm
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
from torchvision import datasets, transforms as T
from torchvision.utils import save_image, make_grid
import matplotlib.pyplot as plt
import numpy as np

# Multi-Head Attention Module
class MultiAttentionLayer(nn.Module):
    def __init__(self, embed_dim, num_heads):
        super().__init__()
        self.head_dim = embed_dim // num_heads
        self.linear_out = nn.Linear(embed_dim, embed_dim)
        self.head_layers = nn.ModuleList([SingleAttention(embed_dim, self.head_dim) for _ in range(num_heads)])

    def forward(self, inputs, attention_mask=None):
        # Calculate attention for each head and concatenate
        attention_output = torch.cat([head(inputs, attention_mask) for head in self.head_layers], dim=-1)
        return self.linear_out(attention_output)

# Transformer Encoder Block
class TransformerBlock(nn.Module):
    def __init__(self, embed_dim, num_heads, mlp_ratio=4):
        super().__init__()
        self.norm1 = nn.LayerNorm(embed_dim)
        self.attention_layer = MultiAttentionLayer(embed_dim, num_heads)
        self.norm2 = nn.LayerNorm(embed_dim)
        self.mlp_layer = nn.Sequential(
            nn.Linear(embed_dim, embed_dim * mlp_ratio),
            nn.GELU(),
            nn.Linear(embed_dim * mlp_ratio, embed_dim)
        )

    def forward(self, x, mask=None):
        # Residual connection for attention + normalization
        x = x + self.attention_layer(self.norm1(x), mask)
        # Residual connection for MLP + normalization
        return x + self.mlp_layer(self.norm2(x))

# Tokenizer Function
def text_tokenizer(text, encode=True, mask=None, max_length=32):
    if encode:
        # Adding special tokens for start and end of text
        processed_text = chr(2) + text + chr(3)
        processed_text = processed_text + "".join([chr(0) for _ in range(max_length - len(processed_text))])
        encoded_text = torch.IntTensor(list(processed_text.encode("utf-8")))

        # Creating attention mask: 1 for text, 0 for padding
        mask = torch.ones(len(encoded_text.nonzero()))
        mask = torch.cat((mask, torch.zeros(max_length - len(mask)))).type(torch.IntTensor)
        return encoded_text, mask
    else:
        # Decode the input, strip out special tokens
        decoded_text = "".join([chr(x) for x in text[1:len(mask.nonzero()) - 1]])
        return decoded_text, None

# Text Encoder Model
class LanguageEncoder(nn.Module):
    def __init__(self, vocab_size, embed_dim, max_seq_len, num_heads, num_layers, proj_dim):
        super().__init__()
        self.embedding = nn.Embedding(vocab_size, embed_dim)
        self.pos_embedding = PositionalEmbedding(embed_dim, max_seq_len)
        self.encoder_layers = nn.ModuleList([TransformerBlock(embed_dim, num_heads) for _ in range(num_layers)])
        self.proj_matrix = nn.Parameter(torch.randn(embed_dim, proj_dim))

    def forward(self, tokens, mask=None):
        x = self.embedding(tokens)
        x = self.pos_embedding(x)
        for layer in self.encoder_layers:
            x = layer(x, mask)
        x = x[torch.arange(tokens.shape[0]), torch.sum(mask[:, 0], dim=1) - 1]
        return x @ self.proj_matrix / torch.norm(x, dim=-1, keepdim=True)

# Image Encoder Model
class VisionEncoder(nn.Module):
    def __init__(self, embed_dim, image_shape, patch_size, num_heads, num_layers, proj_dim):
        super().__init__()
        assert image_shape[0] % patch_size[0] == 0 and image_shape[1] % patch_size[1] == 0, "Image must be divisible by patch size."
        assert embed_dim % num_heads == 0, "Embedding dimension must be divisible by the number of heads."
        
        num_patches = (image_shape[0] * image_shape[1]) // (patch_size[0] * patch_size[1])
        self.patch_embedding = nn.Conv2d(1, embed_dim, kernel_size=patch_size, stride=patch_size)
        self.class_token = nn.Parameter(torch.randn(1, 1, embed_dim))
        self.pos_embedding = PositionalEmbedding(embed_dim, num_patches + 1)
        self.encoder_layers = nn.ModuleList([TransformerBlock(embed_dim, num_heads) for _ in range(num_layers)])
        self.proj_matrix = nn.Parameter(torch.randn(embed_dim, proj_dim))

    def forward(self, images):
        patches = self.patch_embedding(images).flatten(2).transpose(1, 2)
        patches = torch.cat((self.class_token.expand(patches.size(0), -1, -1), patches), dim=1)
        patches = self.pos_embedding(patches)
        for layer in self.encoder_layers:
            patches = layer(patches)
        return patches[:, 0] @ self.proj_matrix / torch.norm(patches[:, 0], dim=-1, keepdim=True)

# Full CLIP Model
class CLIPModel(nn.Module):
    def __init__(self, proj_dim, img_embed_dim, img_shape, patch_dim, num_heads_img, num_layers_img, vocab_size, text_embed_dim, max_len, num_heads_text, num_layers_text):
        super().__init__()
        self.image_encoder = VisionEncoder(img_embed_dim, img_shape, patch_dim, num_heads_img, num_layers_img, proj_dim)
        self.text_encoder = LanguageEncoder(vocab_size, text_embed_dim, max_len, num_heads_text, num_layers_text, proj_dim)
        self.temperature = nn.Parameter(torch.ones([]) * np.log(1 / 0.07))
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    def forward(self, image, text, mask=None):
        img_features = self.image_encoder(image)
        txt_features = self.text_encoder(text, mask)
        logits = (img_features @ txt_features.T) * torch.exp(self.temperature)
        labels = torch.arange(logits.shape[0]).to(self.device)
        loss_img_to_txt = nn.functional.cross_entropy(logits.transpose(-2, -1), labels)
        loss_txt_to_img = nn.functional.cross_entropy(logits, labels)
        return (loss_img_to_txt + loss_txt_to_img) / 2

# FashionMNIST Dataset Class
class FashionMNISTDataset(Dataset):
    def __init__(self, train=True):
        self.data = datasets.FashionMNIST(root='data', train=train, download=True, transform=T.ToTensor())
        self.labels = {0: "t-shirt/top", 1: "trousers", 2: "pullover", 3: "dress", 4: "coat", 5: "sandal", 6: "shirt", 7: "sneaker", 8: "bag", 9: "ankle boot"}

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        img, lbl = self.data[idx]
        cap, mask = text_tokenizer(self.labels[lbl])
        return {"image": img, "caption": cap, "mask": mask}
